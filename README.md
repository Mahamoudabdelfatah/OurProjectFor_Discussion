
## Data Visualization: Multiple Choice Questions (Answers Bolded)

---

1.  According to Dr. Mourad Raafat, why is data visualization essential?
    A. It replaces the need for raw data.















    **B. It transforms complex information into clear, meaningful, and compelling visual representations.**\n
    C. It is only useful for telling data stories.\n
    D. It is primarily for highlighting key insights.\n
    Source:

3.  Data visualization helps enhance understanding by making data-driven arguments more insightful and compelling, similar to how sentences are more persuasive with supporting evidence. This statement is:
    A. False.
    **B. True.**
    C. Only true for numerical data.
    D. Only true for large datasets.
    Source:

4.  Visualizations show data stories by converting numerical, relational, or spatial patterns into images. This is one of the key takeaways for why data visualization is important. This statement is:
    A. False.
    **B. True.**
    C. True, but only for spatial patterns.
    D. True, but only for numerical patterns.
    Source:

5.  A well-designed visualization draws attention to what is most important in the data, making it easier to grasp than raw text alone. This is described as highlighting key insights. This statement is:
    A. False.
    **B. True.**
    C. Only applies to charts, not tables or maps.
    D. Only applies to complex datasets.
    Source:

6.  The sources mention that free and easy-to-use digital tools are available for creating charts, tables, and maps. This is listed as a reason why data visualization is important. This statement is:
    **A. False.**
    C. Only proprietary tools are effective.
    D. Only maps can be created with free tools.
    Source:

7.  Data visualizations support decision-making by helping new learners and professionals decide on the best way to present information. The best way to present information could be through tables, charts, or maps. This statement is:
    A. False.
    **B. True.**
    C. Decision-making is only supported by charts.
    D. Decision-making is only supported by maps.
    Source:

8.  Modern data visualizations can be interactive, allowing users to explore, download, and share insights easily. This is listed as a key benefit of data visualization. This statement is:
    A. False.
    **B. True.**
    C. Interactivity is limited to static visuals.
    D. Sharing insights is not a feature of interactive visualizations.
    Source:

9.  With the rise of digital content, data visualizations reach a wider audience on the web, engaging them in ways that traditional print materials cannot. This is listed under the benefit of increased accessibility. This statement is:
    A. False.
    **B. True.**
    C. Traditional print materials are more engaging than digital visualizations.
    D. Accessibility only refers to sharing insights.
    Source:

10.  Data visualizations can be manipulated to deceive, making it essential to critically evaluate the sources and accuracy of data stories. This is mentioned as a potential negative aspect or challenge of data visualization. This statement is:
    A. False.
    **B. True.**
    C. Visualizations can only uncover the truth.
    D. Evaluating accuracy is not possible with visualizations.
    Source:

11. According to the sources, what is NOT one of the three things you have to understand in order to visualize data?
    A. Preattentive attributes.
    B. Types of data.
    **C. Digital tools.**
    D. Dealing with Colors.
    Sources:

12. Preattentive attributes are things our brains process in milliseconds, before we pay attention to everything else. This definition is provided in the sources. This statement is:
    A. False.
    **B. True.**
    C. They require conscious attention.
    D. They are processed slowly.
    Source:

13. Which of the following is listed as a type of Preattentive Attribute in the sources?
    A. Shape.
    B. Texture.
    **C. Color.**
    D. Orientation.
    Source:

14. The sources give an example showing that coloring every digit when trying to find a specific number (like 9s) is nearly as bad as having no color. This demonstrates a misuse of color. This statement is:
    A. False.
    **B. True.**
    C. Coloring all digits is the best way to highlight them.
    D. Coloring improves the task regardless of how it's applied.
    Source:

15. For quantitative comparisons, what preattentive attributes are mentioned as best?
    A. Color and Size.
    B. Position and Shape.
    **C. Length and Position.**
    D. Size and Texture.
    Source:

16. The sources list three types of data. Which of the following is NOT one of them?
    A. Categorical Data (aka Nominal Data).
    B. Ordinal Data.
    **C. Binary Data.**
    D. Quantitative Data (aka Numerical Data).
    Source:

17. Categorical data is also known as what kind of variable?
    A. Continuous variable.
    **B. Nominal variable.**
    C. Ordinal variable.
    D. Quantitative variable.
    Source:

18. Continuous data is described as allowing for the "in between," meaning there is an infinite number of possible intermediate values. Examples given are Temperature and Weights. This statement is:
    A. False.
    **B. True.**
    C. Continuous data can only take whole number values.
    D. Only Temperature is a type of continuous data.
    Source:

19. According to the sources, color is one of the most important things to understand in data visualization. This statement is:
    A. False.
    **B. True.**
    C. Color is only important for aesthetic purposes.
    D. Color is frequently used correctly.
    Source:

20. The sources advise that you should not use color just to spice up a boring visualization. This suggests that color should be used purposefully. This statement is:
    A. False.
    **B. True.**
    C. Using color to spice up a visualization is always a good practice.
    D. Color should only be used for quantitative data.
    Source:

21. Sequential Color is defined as the use of a single color from light to dark. This definition is provided as a reminder. This statement is:
    A. False.
    **B. True.**
    C. Sequential color uses multiple colors.
    D. Sequential color is only used for categorical data.
    Source:

22. An example of using sequential color given in the sources is encoding the unemployment rate by state. Another example is encoding the total amount of sales by state in blue, where darker blue shows higher sales. This statement is:
    A. False.
    **B. True.**
    C. Sequential color is only for categorical data.
    D. Lighter blue shows higher sales in the example.
    Source:

23. Highlight color is used when there is something that needs to stand out to the reader, but not to alert or alarm them. This description is provided as a reminder. This statement is:
    A. False.
    **B. True.**
    C. Highlight color is primarily for alerting or alarming the reader.
    D. Highlight color is used to make everything stand out equally.
    Source:

24. Which of the following is listed as a way highlights can be used?
    **A. Highlighting a certain data point.**
    B. Highlighting the axis labels.
    C. Highlighting the chart title.
    D. Highlighting the data source.
    Source:

25. According to research mentioned in the sources, approximately what percentage of males have color vision deficiency (CVD)?
    A. 0.4 percent.
    **B. 8 percent.**
    C. 19 percent.
    D. 50 percent.
    Source:

26. The term "color blindness" is not entirely accurate for Color Vision Deficiency (CVD). People suffering from CVD can in fact see color, but cannot distinguish colors in the same way as the rest of the population. This statement is:
    A. False.
    **B. True.**
    C. People with CVD cannot see any color.
    D. People with CVD see colors the same way as everyone else.
    Source:

27. A common issue mentioned regarding CVD is difficulty distinguishing between which two colors?
    A. Blue and Yellow.
    **B. Red and Green.**
    C. Purple and Orange.
    D. Black and White.
    Source:

28. Using red for 'bad' and green for 'good' in a visualization might cause issues for people with CVD. This statement is:
    A. False.
    **B. True.**
    C. This color scheme is universally understandable.
    D. CVD only affects brightness perception, not color.
    Source:

29. Which of the following is listed as a challenge when visualizing multi-class data?
    **A. Managing visual clutter due to numerous classes.**
    B. Increasing the number of dimensions.
    C. Using only one color for all classes.
    D. Reducing accessibility and interpretability.
    Source:

30. Selecting appropriate visualization methods that clearly differentiate classes is a challenge in visualizing multi-class data. This statement is:
    A. False.
    **B. True.**
    C. Any visualization method works well for multi-class data.
    D. Differentiating classes is rarely an issue.
    Source:

31. Issues related to class imbalance or overlapping classes are challenges in multi-class data visualization.
    A. False.
    **B. True.**
    C. Class imbalance simplifies visualization.
    D. Overlapping classes are easy to visualize.
    Source:

32. Choosing suitable colors and graphical elements to ensure accessibility and interpretability by diverse audiences is a challenge in multi-class data visualization. This statement is:
    A. False.
    **B. True.**
    C. Color choice is only an aesthetic consideration.
    D. Accessibility is not relevant for data visualization.
    Source:

33. In a dashboard tracking user engagement by product category, overlapping behaviors from users engaging in multiple categories can make it difficult to isolate trends by class. This is given as an example of a challenge in multi-class visualization. This statement is:
    A. False.
    **B. True.**
    C. User engagement dashboards never face this challenge.
    D. Overlapping behaviors simplify analysis.
    Source:

34. To manage complexity and improve clarity in visualizing overlapping multi-class data, careful use of dimensionality reduction and interactive filtering tools is essential. This advice is given in the context of the user engagement example. This statement is:
    A. False.
    **B. True.**
    C. Adding more dimensions is recommended.
    D. Static visualizations are best for overlapping data.
    Source:

35. A Histogram is a chart that groups numeric data into bins, displaying the bins as segmented columns. This description is given for a Histogram. This statement is:
    A. False.
    **B. True.**
    C. Histograms display individual data points.
    D. Histograms are used for categorical data.
    Source:

36. Histograms are used to depict the distribution of a dataset: how often values fall into ranges. The histogram was first introduced by Karl Pearson. This statement is:
    A. False.
    **B. True.**
    C. Histograms show correlation between two variables.
    D. Karl Pearson introduced the box plot.
    Source:

37. To construct a histogram, the first step is to count how many values fall into each interval, and then bin the range of values. This statement is:
    A. False.
    **B. True.**
    C. The first step is to bin the range of values, then count.
    D. Histograms are not constructed using bins.
    Source:

38. In a histogram, a rectangle is drawn with height proportional to the count and width equal to the bin size. The rectangles abut each other. This statement is:
    A. False.
    **B. True.**
    C. Rectangles in a histogram never touch.
    D. Rectangle width is not related to bin size.
    Source:

39. Box plots can be drawn either horizontally or vertically. This statement is:
    A. False.
    **B. True.**
    C. Box plots can only be drawn vertically.
    D. Box plots can only be drawn horizontally.
    Source:

40. In a box plot diagram, the minimum and maximum values are shown. Outliers, the first quartile (Q1), the third quartile (Q3), and the median are also typically depicted. This statement is:
    A. False.
    **B. True.**
    C. Box plots only show the median and quartiles.
    D. Box plots do not show outliers.
    Source:

41. The Inter Quartile Range (IQR) in a box plot represents the middle 50% of scores for the group. The range of scores from lower to upper quartile is referred to as the inter-quartile range. This statement is:
    A. False.
    **B. True.**
    C. The IQR represents the middle 25% of scores.
    D. The IQR is the range from minimum to maximum.
    Source:

42. Seventy-five percent of the scores fall below the upper quartile (Q3) in a box plot. Twenty-five percent of scores fall below the lower quartile (Q1). This statement is:
    A. False.
    **B. True.**
    C. The upper quartile is the median.
    D. The lower quartile is the minimum.
    Sources:

43. The whiskers in a box plot represent scores outside the middle 50%. They often stretch over a wider range of scores than the middle quartile groups. This statement is:
    A. False.
    **B. True.**
    C. Whiskers always cover a smaller range than the IQR.
    D. Whiskers are always drawn at 1.5 times the IQR.
    Source:

44. The IQR is not shown directly in a box plot graph, but it is a very useful measurement. Why is the IQR useful?
    A. It is influenced by extreme values.
    **B. It limits the range to the middle 50% of the values.**
    C. It shows the mean of the data.
    D. It determines the number of outliers.
    Source:

45. How is the IQR calculated?
    A. Median - Q1.
    **B. Q3 - Q1.**
    C. Q3 + Q1.
    D. Maximum - Minimum.
    Sources:

46. A Scatter plot is a type of mathematical diagram using Cartesian coordinates to display values for two variables for a set of data. The data is displayed as a collection of points. This statement is:
    A. False.
    **B. True.**
    C. Scatter plots display only one variable.
    D. Scatter plots use polar coordinates.
    Source:

47. In a scatter plot, each point's position is determined by the values of the two variables. One variable determines the position on the horizontal axis and the other on the vertical axis. This statement is:
    A. False.
    **B. True.**
    C. Both variables determine the position on only one axis.
    D. The position is determined randomly.
    Source:

48. A Bubble chart is a type of chart that displays how many dimensions of data?
    A. Two dimensions.
    **B. Three dimensions.**
    C. Four dimensions.
    D. One dimension.
    Source:

49. In a bubble chart, how are the three dimensions of data represented?
    A. Two values through xy location, one through color.
    B. One value through x, one through y, one through color.
    **C. Two values through xy location, one through size.**
    D. All three values through size.
    Source:

50. Bubble charts can be considered a variation of which other chart type?
    A. Bar chart.
    B. Line chart.
    **C. Scatter plot.**
    D. Pie chart.
    Source:

51. A Heat map is a data visualization type where the individual values contained in a matrix are represented through variations in what?
    A. Size.
    B. Shape.
    **C. Coloring.**
    D. Position.
    Source:

52. The term "Heat map" was originally introduced by software designer Cormac Kinney in 1991. This statement is:
    A. False.
    **B. True.**
    C. Heat maps existed for over a century before 1991.
    D. Karl Pearson introduced the heat map.
    Source:

53. According to the sources, to select the right type of visual chart, you need to match your data with the chart. This requires asking several questions. Which of the following is NOT listed as a question to ask?
    A. What type of data?.
    B. How do you want to communicate?.
    **C. What software should I use?.**
    D. Who is my audience?.
    Source:

54. When comparing charts, Tables are recommended for which purpose?
    A. To display trends over time.
    **B. To compare individual or precise values.**
    C. To show the rank of values and focus on the extremes.
    D. To visualize comparisons of quality data.
    Source:

55. Tables are useful when displaying quantitative information is more important than trends. This statement is:
    A. False.
    **B. True.**
    C. Tables are always better than charts for showing trends.
    D. Tables cannot display quantitative information.
    Source:

56. Column Chart and Cluster column bar charts are recommended for which purpose?
    A. To display parts of a whole in percentages.
    B. To show if data values have attained a particular goal.
    **C. To display and compare the rank of values and focus on the extremes.**
    D. To compare multiple items or groups on various attributes.
    Source:

57. Column charts and Cluster column bar charts are suitable for short data category labels and when items on the chart have less than seven categories. This statement is:
    A. False.
    **B. True.**
    C. They are best for long labels.
    D. They work best with more than 15 categories.
    Source:

58. Bar charts are recommended for which purpose?
    A. To display parts of a whole in percentages.
    **B. To show if the data values have attained a particular goal.**
    C. To compare individual or precise values.
    D. To display trends over time.
    Source:

59. Bar charts are suitable for items on the chart having how many categories?
    A. Less than seven.
    **B. Over seven but less than 15.**
    C. Exactly three.
    D. Any number of categories.
    Source:

60. Bar charts can be used to display negative numbers and are suitable for long data category labels. This statement is:
    A. False.
    **B. True.**
    C. Bar charts can only display positive numbers.
    D. Bar charts are only for short labels.
    Source:

61. Radar charts are recommended for which purpose?
    A. To display parts of a whole in percentages.
    **B. To compare multiple items or groups on various attributes.**
    C. To show if the data values have attained a particular goal.
    D. To display and compare the rank of values.
    Source:

62. For Radar charts, the number of attributes compared should be at least three but less than 10. This statement is:
    A. False.
    **B. True.**
    C. Any number of attributes can be used.
    D. They are best for exactly two attributes.
    Source:

63. Pie Charts are recommended for which purpose?
    A. To compare many different items.
    **B. To display parts of a whole in percentages.**
    C. To show trends over time.
    D. To visualize comparisons of quality data.
    Source:

64. In Pie Charts, all categories must add up to what total percentage?
    A. 50%.
    B. 75%.
    **C. 100%.**
    D. Any percentage is acceptable.
    Source:

65. Stacked Bar charts are recommended for which purpose?
    A. To display parts of a whole in percentages.
    B. To compare multiple items or groups on various attributes.
    **C. To compare many different items and show the composition of each.**
    D. To display the rank of values.
    Source:

66. Stacked Bar charts can be used to display visual aggregate all of the categories in a group when the size of individual categories is not important. This statement is:
    A. False.
    **B. True.**
    C. Stacked Bar charts are only useful when individual category size is crucial.
    D. Stacked Bar charts do not show composition.
    Source:

67. Dimensionality Reduction is needed due to several issues that arise with high dimensional data. Which of the following is listed as one of these issues?
    A. Ease in clustering similar features.
    B. Decreased space and computational time complexity.
    C. Running a risk of overfitting the machine learning model.
    **D. Ability to easily visualize/understand the data.**
    Source:

68. High dimensional data can be difficult to visualize or understand. Dimensionality reduction techniques can help address this issue by reducing the number of features. This statement is:
    A. False.
    **B. True.**
    C. High dimensional data is always easy to visualize.
    D. Dimensionality reduction increases the number of features.
    Sources:

69. Which of the following is listed as a Decomposition algorithm for dimensionality reduction?
    A. `t-Distributed Stochastic Neighbor Embedding (t-SNE)`.
    B. `Linear Discriminant Analysis (LDA)`.
    **C. Principal Component Analysis (PCA).**
    D. `Locally Linear Embedding (LLE)`.
    Source:

70. Which of the following is listed as a Manifold learning algorithm?
    A. `Principal Component Analysis (PCA)`.
    B. `Singular Value Decomposition (SVD)`.
    **C. t-Distributed Stochastic Neighbor Embedding (t-SNE).**
    D. `Kernel Principal Component Analysis (Kernel PCA)`.
    Source:

71. Which of the following is listed as a Discriminant Analysis algorithm?
    A. `Spectral Embedding`.
    **B. Quadratic Discriminant Analysis (QDA).**
    C. `Non-Negative Matrix Factorization (NMF)`.
    D. `Locally Linear Embedding (LLE)`.
    Source:

72. Dimensionality reduction techniques could be categorized between applied on linear and non-linear distributions. Non-linear distributions describe a situation where there is not a straight-line or direct relationship between variables. This statement is:
    A. False.
    **B. True.**
    C. Dimensionality reduction only applies to linear relationships.
    D. Non-linear relationships are always straight lines.
    Source:

73. Principal Component Analysis (PCA) is a dimensionality-reduction method to find lower-dimensional space by preserving the variance as measured in the high dimensional input space. This is given as a summary of PCA. This statement is:
    A. False.
    **B. True.**
    C. PCA maximizes the mean, not variance.
    D. PCA increases the dimensions.
    Source:

74. PCA is described as what type of method for dimensionality reduction?
    A. Supervised method.
    **B. Unsupervised method.**
    C. Semi-supervised method.
    D. Classification method.
    Source:

75. PCA transformations are linear transformations. This statement is:
    A. False.
    **B. True.**
    C. PCA only performs non-linear transformations.
    D. The type of transformation depends on the data.
    Source:

76. One of the steps in the process of PCA mentioned is the standardization of the data. Standardization transforms data such that the resulting distribution has a mean of 0 and a standard deviation of 1. This statement is:
    A. False.
    **B. True.**
    C. Standardization changes the mean but not the standard deviation.
    D. Standardization is not a step in PCA.
    Source:

77. The covariance matrix is calculated in PCA. It is an `n x n` matrix, where `n` represents the dimensions of the dataset. The covariance value denotes how co-dependent two variables are. This statement is:
    A. False.
    **B. True.**
    C. The covariance matrix is always `2x2`.
    D. Covariance measures independence, not co-dependence.
    Source:

78. In PCA, you calculate the eigenvectors and eigenvalues from the covariance matrix. Eigenvalues determine the radius of the Ellipse. This statement is:
    **A. False.**
    B. True.
    C. Eigenvalues determine the direction of variance.
    D. Eigenvalues and eigenvectors are not used in PCA.
    Source:

79. The idea behind eigenvectors in PCA is to use the Covariance matrix to understand where in the data there is the most amount of variance. For every eigenvector, there is an eigenvalue. This statement is:
    A. False.
    **B. True.**
    C. Eigenvectors show the mean of the data.
    D. Eigenvectors and eigenvalues are unrelated.
    Source:

80. The final step in computing the Principal Components is to form a matrix known as the feature matrix that contains all the significant data variables. This matrix is obtained using eigenvectors and the original data. This statement is:
    **A. False.**
    B. True.
    C. The feature matrix contains random data variables.
    D. The feature matrix is formed before calculating eigenvectors.
    Sources:

81. To reduce the dimensions of the dataset in PCA, you project the feature space onto a smaller subspace. The eigenvectors will form the axes of this new feature subspace. This statement is:
    A. False.
    **B. True.**
    C. The original features form the axes of the new subspace.
    D. PCA does not involve projecting the feature space.
    Source:

82. How do you typically choose the number of components (k) in PCA for visualization purposes?
    A. `k = 1`.
    **B. k = 2.**
    C. `k = 3`.
    D. `k = 50`.
    Source:

83. When choosing the number of components (k) in PCA, typically you want the explained variance to be between what percentage?
    A. 50-75%.
    B. 80-90%.
    **C. 95-99%.**
    D. 100%.
    Source:

84. In Python, when implementing PCA, you can specify the number of components (k) or specify a percentage of explained variance (e.g., `components = 0.95`). This statement is:
    A. False.
    **B. True.**
    C. Only the number of components can be specified.
    D. Explained variance is not relevant in Python PCA.
    Source:

85. In a more dynamic PCA application described, PCA is applied on the train data with a certain variance percentage, then a model is applied, and finally, the testing set is transformed and prediction is applied. This statement is:
    A. False.
    **B. True.**
    C. PCA is only applied to the testing set.
    D. The model is applied before PCA.
    Source:

86. Kernel Principal Component Analysis (Kernel PCA) is used when dealing with what type of distributions?
    A. Linear distributions.
    **B. Non-linear distributions.**
    C. Uniform distributions.
    D. Normal distributions.
    Source:

87. The idea behind Kernel PCA is to try to increase the dimensions, then apply PCA. This is done to find a linear separation in a higher dimensional space. This statement is:
    A. False.
    **B. True.**
    C. Kernel PCA decreases dimensions first.
    D. Kernel PCA does not involve applying PCA.
    Sources:

88. The RBF kernel function (Gaussian Kernel) is used in Kernel PCA to compute the similarity or how close two points are to each other. When points are the same, the distance is close to 1. This statement is:
    A. False.
    **B. True.**
    C. The RBF kernel measures dissimilarity.
    D. When points are the same, the kernel value is close to 0.
    Sources:

89. In Kernel PCA using landmarks, each point can be a landmark. If `x` is close to a landmark, the similarity is close to 1; if far, the similarity is close to 0. This statement is:
    A. False.
    **B. True.**
    C. Landmarks are chosen randomly.
    D. Similarity is always close to 0.
    Sources:

90. After applying the Kernel function in Kernel PCA, you can apply PCA. This is done because the Kernel transformed the data into a space where a linear separation might be possible. This statement is:
    A. False.
    **B. True.**
    C. PCA is not applied after the Kernel.
    D. The Kernel itself provides the final linear separation.
    Source:

91. Singular Value Decomposition (SVD) is a factorization method that is efficient when working with what type of dataset?
    A. Dense dataset.
    **B. Sparse dataset (having many zero entries).**
    C. Small dataset.
    D. Continuous dataset.
    Source:

92. Examples of datasets where SVD is typically used include data for a recommender system or a bag of words model for text. This is because these datasets are often sparse. This statement is:
    A. False.
    **B. True.**
    C. SVD is best for dense datasets like recommender systems.
    D. Bag of words models are typically dense.
    Source:

93. The idea of SVD is that every matrix of shape `n x p` factorizes into `A = USV^T`. `U` is an orthogonal matrix, `S` is a diagonal matrix, and `V^T` is also an orthogonal matrix. This statement is:
    A. False.
    **B. True.**
    C. `U`, `S`, and `V^T` are all diagonal matrices.
    D. The factorization is `A = USV`.
    Sources:

94. In the SVD factorization `A = USV^T`, if `A` is an `m x n` input data matrix, what is the shape of the `U` matrix (Left singular Vectors)?
    A. `n x n`.
    **B. m x r (where r is the rank of A).**
    C. `r x r`.
    D. `n x r`.
    Source:

95. In the SVD factorization `A = USV^T`, `Σ` (or `S`) is a diagonal matrix. What does it represent?
    A. Left singular vectors.
    B. Right singular vectors.
    **C. Singular values (strength of each concept).**
    D. The input data matrix.
    Source:

96. According to the sources, if the data is dense, it is better to use which dimensionality reduction method?
    A. SVD.
    **B. PCA.**
    C. Kernel PCA.
    D. t-SNE.
    Source:

97. PCA can be implemented as SVD on the covariance matrix. SVD and PCA are for linear distributions. This statement is:
    A. False.
    **B. True.**
    C. PCA is for non-linear distributions.
    D. SVD is not related to PCA.
    Source:

98. The sources state that PCA and SVD are almost the same, but the difference is that PCA calculates the eigenvalues from the `COV` matrix, whereas SVD tries to find a decomposition of the data itself. This statement is:
    A. False.
    **B. True.**
    C. PCA calculates eigenvalues from the data matrix directly.
    D. SVD calculates eigenvalues from the covariance matrix.
    Source:

99. In the context of the Anscombe's quartet data presented visually, Group A shows data points that appear to be:
    **A. Strongly linearly correlated.**
    B. Non-linearly correlated.
    C. Not strongly correlated.
    D. Grouped in a circle.
    Source:

100. In the visual analysis of the Anscombe's quartet data, Group B initially shows a linear trend, but after a certain point (e.g., X=10), the trend appears to change. This statement is:
    A. False.
    **B. True.**
    C. Group B always shows a strong linear trend.
    D. Group B shows no clear trend.
    Source:

101. According to the explanation of visualizing the Anscombe's quartet data, simply looking at the raw table of numbers is usually less insightful than visualizing the data. This statement is:
    A. False.
    **B. True.**
    C. Raw tables are always easier to understand.
    D. Visualization makes the data harder to understand.
    Source:

102. The size of a visual element can be used as a preattentive attribute to draw the viewer's attention to something specific. This is mentioned as a tool available to the visualizer. This statement is:
    A. False.
    **B. True.**
    C. Size is not a preattentive attribute.
    D. Size should only be used for aesthetic purposes.
    Source:

103. Quantitative data, which are numbers, can be one of two types: Discrete or Continuous. This classification is presented in the sources. This statement is:
    A. False.
    **B. True.**
    C. Quantitative data is always continuous.
    D. Quantitative data is always discrete.
    Source:

104. Discrete quantitative data takes specific, countable values, like the number of courses taken in a semester (e.g., 4, 5, 6). You cannot have 4.5 courses. This statement is:
    A. False.
    **B. True.**
    C. Discrete data can take any value within a range.
    D. The example given is for continuous data.
    Source:

105. A Bar chart is given as an example visualization method suitable for discrete quantitative data, such as the number of courses students registered for. This statement is:
    A. False.
    **B. True.**
    C. Bar charts are only for continuous data.
    D. Histograms are better for discrete data.
    Source:

106. In the example of visualizing malaria deaths in Africa, the variables discussed were Country, District, and Year. Which of these was described as Categorical (Nominal)?
    **A. Country.**
    B. District.
    C. Year.
    D. All three.
    Source:

107. In the malaria death example, the Year variable was described as potentially Ordinal. Why?
    A. Because you cannot add years together.
    B. Because it is a numerical variable.
    **C. Because you can order the years sequentially.**
    D. Because it is a nominal variable.
    Source:

108. When visualizing the malaria death data using a format like a heatmap (where color intensity indicates value), the color scale allows the viewer to understand the range of values being represented. This key (or legend) is important for interpretation. This statement is:
    A. False.
    **B. True.**
    C. The color intensity is self-explanatory without a key.
    D. Heatmaps do not use color scales.
    Source:

109. Visualizing geographical data on a map allows the viewer to leverage their existing knowledge of locations. This is mentioned as a benefit of using maps. This statement is:
    A. False.
    **B. True.**
    C. Maps make it harder to understand geographical data.
    D. Maps are only useful for showing trends over time.
    Source:

110. Interactive visualizations allow users to drill down into the data for more detail. For example, clicking on a region might show data for specific years. This statement is:
    A. False.
    **B. True.**
    C. Interactive visualizations limit data exploration.
    D. Interactivity is only for aesthetic purposes.
    Source:

111. A dashboard is presented as a way to combine multiple visualizations (like charts, maps, etc.) on a single screen. This allows a manager to view related information together. This statement is:
    A. False.
    **B. True.**
    C. Dashboards only show one visualization at a time.
    D. Dashboards are not useful for managers.
    Source:

112. Using color in a choropleth map (like the unemployment example) can highlight areas of high or low values. The color red is associated with high unemployment rates in one example. This statement is:
    A. False.
    **B. True.**
    C. Color is not effective for choropleth maps.
    D. Red is always associated with low values.
    Source:

113. Sequential color is used to show varying degrees of a single measure, like unemployment rate or total sales, using different shades of the same color. This reinforces the concept of sequential color. This statement is:
    A. False.
    **B. True.**
    C. Sequential color uses many different colors.
    D. Sequential color is only for categorical data.
    Sources:

114. Highlight color can be used to draw attention to a specific data point, text in a table, a line on a line chart, or a bar on a bar chart. This is done without necessarily implying the highlighted item is bad or alarming. This statement is:
    A. False.
    **B. True.**
    C. Highlight color is only for alarming information.
    D. Highlight color cannot be used on lines or bars.
    Sources:

115. Alarm color uses visual cues (like red or orange circles) to indicate the status or recency of information, such as highlighting old or potentially inaccurate data. This statement is:
    A. False.
    **B. True.**
    C. Alarm color is only used for positive information.
    D. Alarm color is the same as highlight color.
    Source:

116. Color vision deficiency (CVD) can make it difficult for individuals to distinguish between certain colors, particularly red and green. Visualizations relying solely on these colors to convey different meanings (like good/bad) can be problematic. This statement is:
    A. False.
    **B. True.**
    C. CVD only affects blue and yellow perception.
    D. Using red and green is always safe for all audiences.
    Source:

117. The source suggests that using simulations can help visualize what a chart looks like to someone with color vision deficiency. This is a tool to improve accessibility. This statement is:
    A. False.
    **B. True.**
    C. Simulations are not useful for testing accessibility.
    D. CVD cannot be simulated.
    Source:

118. In a bar chart displaying favorite colors among children, the height of the bars effectively shows which color is most popular. This relies on the principle that length is good for quantitative comparisons. This statement is:
    A. False.
    **B. True.**
    C. The color of the bar is more important than its length.
    D. Bar charts are not good for comparing quantities.
    Sources:

119. A Line chart is most useful for showing trends over what?
    A. Over groups.
    B. Over multiple attributes.
    **C. Over time.**
    D. Over categories.
    Sources:

120. Multiple line charts plotted on the same axes are effective for comparing trends across different categories or groups over time. An example given is comparing the population trends of different animal species (bears, dolphins, whales). This statement is:
    A. False.
    **B. True.**
    C. Multiple line charts are difficult to compare.
    D. Line charts cannot show trends for multiple groups.
    Source:

121. A Scatter plot is primarily used to visualize the relationship between how many variables for a set of data?
    A. One variable.
    **B. Two variables.**
    C. Three variables.
    D. More than three variables.
    Source:

122. In a scatter plot, each data point represents a single observation or entity. The position of the point indicates the values of the two variables for that observation. This statement is:
    A. False.
    **B. True.**
    C. Scatter plots aggregate data points.
    D. Points in a scatter plot do not represent individual observations.
    Sources:

123. A scatter plot visualizing apartment price versus size can help identify trends or lack thereof between these two variables. It can show if larger apartments tend to have higher prices. This statement is:
    A. False.
    **B. True.**
    C. Scatter plots cannot show trends.
    D. This example is better suited for a bar chart.
    Sources:

124. The source mentions that if you have multiple categories in a scatter plot, you can differentiate them using different colors or symbols as preattentive attributes. This makes it easier to distinguish which points belong to which category. This statement is:
    A. False.
    **B. True.**
    C. All categories must use the same color and symbol.
    D. Preattentive attributes are not useful in scatter plots.
    Sources:

125. A Dot plot displays the distribution of a single variable. Each point represents an individual observation, and points are stacked to show frequency at specific values. This statement is:
    A. False.
    **B. True.**
    C. Dot plots show the relationship between two variables.
    D. Dot plots use bins like histograms.
    Source:

126. A dot plot is given as suitable for visualizing the number of books read by several students, where each point is a student. This shows the frequency of students reading a certain number of books. This statement is:
    A. False.
    **B. True.**
    C. This example is only suitable for a scatter plot.
    D. A dot plot cannot show frequencies.
    Source:

127. Compared to a bar chart, a dot plot might be preferred when visualizing the distribution of a single variable with a relatively small number of data points or unique values. It gives a sense of the individual points. This statement is:
    A. False.
    **B. True.**
    C. Bar charts are always better than dot plots.
    D. Dot plots are best for thousands of data points.
    Source:

128. A scatter plot can be used to compare two different measures for the same set of entities, such as the number of books read and the number of pages read by each student. This shows the relationship between these two measures. This statement is:
    A. False.
    **B. True.**
    C. Scatter plots can only compare unrelated measures.
    D. This requires a dot plot, not a scatter plot.
    Source:

129. A Choropleth map uses variations in color intensity to represent the value of a variable across geographical regions, such as states or countries. This allows for quick visual comparison of values across locations. This statement is:
    A. False.
    **B. True.**
    C. Choropleth maps use different symbols, not colors.
    D. Choropleth maps show individual data points, not aggregated values for regions.
    Sources:

130. A Bubble chart is presented again as a way to visualize data on a map, where the size of the bubble represents a quantitative value associated with a geographical location. An example given is showing the intensity or count of hurricanes. This statement is:
    A. False.
    **B. True.**
    C. Bubble size on a map is only for categorical data.
    D. This requires a choropleth map, not a bubble chart.
    Source:

131. A Bullet graph is a chart type used to compare an actual value against a target value and show performance relative to defined performance bands (like Bad, Satisfactory, Good). This statement is:
    A. False.
    **B. True.**
    C. Bullet graphs only show the target value.
    D. Bullet graphs are used for comparing trends over time.
    Source:

132. In a Bullet graph, the actual value is typically represented by a bar or line, the target value by a line, and the performance bands by colored background segments. This statement is:
    A. False.
    **B. True.**
    C. Performance bands are represented by separate lines.
    D. The target value is shown as a bar.
    Sources:

133. A key benefit of the bullet graph mentioned is that it allows a quick visual assessment of whether the actual value has met or exceeded the target and how performance falls within defined ranges. This statement is:
    A. False.
    **B. True.**
    C. Bullet graphs are complex and hard to interpret quickly.
    D. Bullet graphs only show the actual value.
    Source:

134. Stacked bullet graphs can be used to compare the performance (actual vs target) of multiple items or categories simultaneously within a single visualization. This allows for easy comparison across items. This statement is:
    A. False.
    **B. True.**
    C. Each item requires a separate bullet graph.
    D. Stacked bullet graphs do not show targets.
    Source:

135. In one example of a stacked bullet graph visualizing sales performance by state, states that met or exceeded the target were highlighted using a specific color (blue). This is another application of highlighting. This statement is:
    A. False.
    **B. True.**
    C. States exceeding the target were colored red.
    D. Highlighting is not used in bullet graphs.
    Sources:

136. A Pie chart is described as displaying parts of a whole in percentages. It shows the relative proportion of each category. This statement is:
    A. False.
    **B. True.**
    C. Pie charts show absolute values, not percentages.
    D. Pie charts compare multiple items or groups.
    Source:

137. A Stacked Bar chart is used to compare many different items and show the composition of each item being compared. It displays the visual aggregate of all categories in a group. This statement is:
    A. False.
    **B. True.**
    C. Stacked bar charts only show the total value for each item.
    D. Stacked bar charts are not used for comparison.
    Source:

138. The sources discuss that it can be difficult to accurately determine the percentage represented by each slice in a standard Pie chart, especially when slices are similar in size or oriented at different angles. This is highlighted as a potential drawback. This statement is:
    A. False.
    **B. True.**
    C. Pie charts are always easy to read precisely.
    D. The angle of a slice does not affect perception.
    Source:

139. A Donut chart is mentioned as an alternative to a Pie chart. What feature of the Donut chart is highlighted as potentially improving readability compared to a Pie chart?
    A. It uses more colors.
    **B. It aligns the segments, making length comparison easier.**
    C. It shows absolute values instead of percentages.
    D. It can display more dimensions.
    Source:

140. An Area chart is described as useful for showing trends over time for multiple series while also showing the composition of the whole. It combines aspects of a line chart and a stacked bar chart. This statement is:
    A. False.
    **B. True.**
    C. Area charts are only for single series data.
    D. Area charts do not show composition.
    Source:

141. A Radar chart is used for comparing multiple items or groups across various quantitative attributes. It can also be used for quality data comparisons. This statement is:
    A. False.
    **B. True.**
    C. Radar charts are only for showing trends over time.
    D. Radar charts are only for qualitative data.
    Sources:

142. The source emphasizes that the design of a chart is very important. Knowing how to use tools is necessary, but knowing how to design the information visually makes a difference. This statement is:
    A. False.
    **B. True.**
    C. Design is less important than the data itself.
    D. Anyone can design a chart equally well.
    Sources:

143. In the context of data visualization, the "quality" of the visualization often refers to how effectively and quickly it conveys information to the intended audience. Key goals are to get information across as much as possible and understand it in the least amount of time. This statement is:
    A. False.
    **B. True.**
    C. Quality is only about aesthetic appeal.
    D. Speed of understanding is not a goal.
    Source:

144. What are mentioned as important data properties besides the mean?
    A. Mode, Median, Range, Outliers.
    **B. Variance, Covariance, Correlation, Distribution.**
    C. Standard Deviation, IQR, Skewness, Kurtosis.
    D. Min, Max, Q1, Q3.
    Source:

145. Mean alone is not sufficient to understand a dataset's characteristics. Why is variance important?
    A. It shows the average value.
    **B. It shows the spread or variation from the mean.**
    C. It shows the relationship between variables.
    D. It shows the distribution shape.
    Sources:

146. Covariance and Correlation are important data properties because they show the relationship between different variables. They indicate whether variables tend to increase or decrease together. This statement is:
    A. False.
    **B. True.**
    C. Covariance and Correlation only describe single variables.
    D. They show the center of the data, not relationships.
    Source:

147. Visualizing data with more than three dimensions (like 4 or more features) can be challenging or impossible directly with standard charts like 2D/3D scatter plots. This is why dimensionality reduction is needed. This statement is:
    A. False.
    **B. True.**
    C. Any number of dimensions can be plotted in a 3D scatter plot.
    D. Visualizing 4D data is as easy as 2D data.
    Sources:

148. PCA can be used to reduce the number of features (dimensions) in a dataset to make it suitable for visualization. For example, reducing 4 features to 2 or 3 allows plotting in 2D or 3D scatter plots. This statement is:
    A. False.
    **B. True.**
    C. PCA increases the number of features for visualization.
    D. PCA is not used for visualization purposes.
    Sources:

149. The Iris dataset is used as an example with 3 classes (types of flowers) and 4 features (measurements like sepal/petal length/width). It has 150 samples, with 50 samples per class. This statement is:
    A. False.
    **B. True.**
    C. The Iris dataset has 4 classes and 3 features.
    D. The Iris dataset is an example of sparse data.
    Sources:

150. The goal of visualizing the Iris dataset features after PCA is to see if the chosen features are suitable for distinguishing (classifying) the different classes. A good visualization would show clear separation between the classes. This statement is:
    A. False.
    **B. True.**
    C. Visualization is not used to assess feature suitability for classification.
    D. Good features would show significant overlap between classes.
    Sources:

151. In the 2D PCA visualization of the Iris dataset, the classes appeared well-separated. One class (blue) was completely separated from the others, while the other two had some minor overlap. This statement is:
    A. False.
    **B. True.**
    C. All classes were completely overlapping.
    D. None of the classes were separated.
    Sources:

152. The sources introduce the fundamental equation for eigenvalues and eigenvectors: `A * x = lambda * x`. Here, `A` is a matrix, `x` is the eigenvector, and `lambda` is the eigenvalue. This statement is:
    A. False.
    **B. True.**
    C. `A` is a scalar, not a matrix.
    D. `Lambda` is a vector, not a scalar.
    Source:

153. To find the eigenvalues (`lambda`) of a matrix `A`, you need to solve the characteristic equation: `determinant(A - lambda * I) = 0`. Here, `I` is the identity matrix. This statement is:
    A. False.
    **B. True.**
    C. You solve for `lambda` using `A + lambda * I = 0`.
    D. Determinants are not used to find eigenvalues.
    Sources:

154. Once eigenvalues (`lambda`) are found, to find the corresponding eigenvectors (`x`), you need to solve the system of linear equations: `(A - lambda * I) * x = 0`. This statement is:
    A. False.
    **B. True.**
    C. You solve `(A + lambda * I) * x = 0`.
    D. Eigenvectors are found independently of eigenvalues.
    Source:

155. The source demonstrates simplifying the matrix `(A - lambda * I)` using row operations to get it into a form that makes solving for `x` easier, like the 'E Q Z' form (الاي كيو زيرو). This form helps deduce the eigenvector components. This statement is:
    A. False.
    **B. True.**
    C. Row operations are not used in finding eigenvectors.
    D. The E Q Z form makes solving for `x` impossible.
    Source:

156. The sources explain that if, after simplifying the matrix `(A - lambda * I)`, you get a row of zeros, it indicates that the equations are dependent, and you can assign a free variable (like `k`) to one of the eigenvector components. This is typical when solving for eigenvectors. This statement is:
    A. False.
    **B. True.**
    C. A row of zeros means there are no solutions for `x`.
    D. You must solve for every component directly without free variables.
    Sources:

157. The sources demonstrate solving for eigenvector components (`x1`, `x2`, `x3`) by expressing them in terms of a free variable (`k`) after simplifying the matrix. This results in an eigenvector that is a multiple of a base vector. This statement is:
    A. False.
    **B. True.**
    C. All eigenvector components must be fixed numbers.
    D. The free variable method only works sometimes.
    Sources:

158. If, after simplifying the matrix, you get a row like `[0 0 Z9]` where `Z9` is non-zero, and this corresponds to the third component (`x3`), what does this imply about `x3` in the eigenvector solution?
    A. `x3` can be any value.
    **B. x3 must be 0.**
    C. `x3` must be 1.
    D. This implies nothing about `x3`.
    Source:

159. The sources note that the covariance matrix is symmetric. This means that the covariance between variable 1 and variable 2 is the same as the covariance between variable 2 and variable 1. This statement is:
    A. False.
    **B. True.**
    C. The covariance matrix is always diagonal.
    D. Symmetry does not apply to covariance matrices.
    Source:

160. If the covariance matrix is diagonal (meaning all off-diagonal elements are zero), it indicates that the features (variables) are what?
    A. Highly correlated.
    B. Highly co-dependent.
    **C. Not correlated or independent.**
    D. All zeros.
    Source:

161. In Machine Learning and PCA, having features that are not correlated (covariance is zero) is generally considered a good property. Why?
    A. It adds redundant information.
    **B. It reduces redundant information, simplifying calculations and understanding.**
    C. It increases the number of necessary dimensions.
    D. It leads to overfitting.
    Sources:

162. PCA works by finding new axes (defined by eigenvectors) that capture the most variance in the data. Rotating the axes can reveal uncorrelated views of the data. This statement is:
    A. False.
    **B. True.**
    C. PCA only works on the original axes.
    D. PCA aims to minimize variance.
    Sources:

163. A key step in PCA calculations shown is subtracting the mean from each data point (standardization or centering). This ensures the mean of the centered data is zero. This statement is:
    A. False.
    **B. True.**
    C. Subtracting the mean is not a step in PCA.
    D. Centering changes the variance of the data.
    Sources:

164. The covariance matrix can be calculated from the centered data matrix (`X'`) as `(X')^T * X'`. This is a shortcut mentioned in the sources. This statement is:
    A. False.
    **B. True.**
    C. The covariance matrix is calculated as `X' * (X')^T`.
    D. This shortcut is only for specific datasets.
    Sources:

165. After calculating the eigenvalues in PCA, you order them from largest to smallest. The eigenvectors corresponding to the largest eigenvalues capture the most variance and are the most important components. This statement is:
    A. False.
    **B. True.**
    C. Eigenvalues are ordered from smallest to largest.
    D. The order of eigenvalues does not matter.
    Source:

166. Normalizing the eigenvectors means scaling them so they have a length (magnitude) of 1. This is often done in PCA implementation. This statement is:
    A. False.
    **B. True.**
    C. Eigenvectors are never normalized.
    D. Normalization changes the direction of the eigenvector.
    Sources:

167. Truncation in PCA refers to the process of removing components (eigenvectors and their corresponding eigenvalues) that contribute less to the total variance. This is how dimensionality is reduced. This statement is:
    A. False.
    **B. True.**
    C. Truncation means adding more components.
    D. Truncation is not a part of dimensionality reduction.
    Source:

168. The concept of 'Explained Variance' in PCA refers to how much of the original data's variance is preserved after reducing the dimensions. Choosing components based on explained variance ensures minimal information loss. This statement is:
    A. False.
    **B. True.**
    C. Explained variance measures the mean preserved.
    D. Higher explained variance means more information is lost.
    Sources:

169. If PCA produces an eigenvalue of zero, it implies that the corresponding eigenvector captures zero variance. This component (and dimension) could be considered redundant and removed without losing any variance. This statement is:
    A. False.
    **B. True.**
    C. A zero eigenvalue means the component captures maximum variance.
    D. Components with zero eigenvalues must always be kept.
    Source:

170. When visualizing multi-class data without pre-existing labels for each data point, you might use algorithms like K-Means clustering to automatically find groups (clusters) in the data. These clusters can then be treated as classes. This statement is:
    A. False.
    **B. True.**
    C. K-Means requires labels as input.
    D. Clustering algorithms are not useful for unlabelled data.
    Source:

171. K-Means clustering is described as an algorithm that takes data and the desired number of clusters (k) as input. It outputs the clusters (or predicted labels) and the cluster centers (centroids). This statement is:
    A. False.
    **B. True.**
    C. K-Means outputs only the cluster labels.
    D. K-Means requires cluster centers as input.
    Sources:

172. One of the steps in the K-Means algorithm is assigning each data point to the cluster whose center is closest to it. The squared Euclidean distance is mentioned as a way to measure closeness. This statement is:
    A. False.
    **B. True.**
    C. K-Means assigns points randomly.
    D. K-Means only uses Manhattan distance.
    Sources:

173. After assigning points to clusters in K-Means, the algorithm updates the cluster centers by calculating the average (mean) of all points assigned to each cluster. This is an iterative process that repeats until the centers stabilize or points stop changing clusters. This statement is:
    A. False.
    **B. True.**
    C. Cluster centers are updated randomly.
    D. The process stops after the first iteration.
    Sources:

174. Visualizing multi-class data can involve coloring each data point according to its class (if labels are available) or its predicted cluster (if using clustering). This is typically done using a scatter plot. This statement is:
    A. False.
    **B. True.**
    C. Data points are never colored based on class.
    D. Only cluster centers are visualized.
    Sources:

175. For high-dimensional multi-class data (like the digits dataset with 64 features), dimensionality reduction using PCA is necessary before visualizing the data points on a 2D or 3D scatter plot. This statement is:
    A. False.
    **B. True.**
    C. High-dimensional data can be visualized directly in 2D.
    D. Dimensionality reduction is not needed for multi-class data.
    Sources:

176. One visualization approach for multi-class data discussed is drawing bounding polygons (like convex hulls) around each cluster or class. This visually defines the area occupied by each group. This statement is:
    A. False.
    **B. True.**
    C. Bounding polygons show the individual data points.
    D. Polygons are only for single-class data.
    Sources:

177. Another visualization approach is background segmentation, where the entire visualization area (background) is colored based on the predicted class for every point in that area. The actual data points can then be plotted on top. This statement is:
    A. False.
    **B. True.**
    C. Background segmentation is done before clustering.
    D. Data points are not shown when using background segmentation.
    Sources:

178. To perform background segmentation, a grid of points is created across the visualization space. For each grid point, a classification or clustering model is used to predict which class it belongs to. This prediction determines the color for that point in the background. This statement is:
    A. False.
    **B. True.**
    C. Background segmentation colors areas randomly.
    D. Only actual data points are used to color the background.
    Source:

179. The smoothness of the background segmentation visualization depends on the density of the grid points. A denser grid (smaller step size) results in a smoother visual separation between classes. This statement is:
    A. False.
    **B. True.**
    C. Grid density does not affect smoothness.
    D. A larger step size gives a smoother result.
    Sources:

180. The sources compare applying K-Means clustering directly to the raw, high-dimensional data versus applying K-Means to the PCA-reduced data. What trade-off is discussed between these two approaches?
    A. Accuracy vs. Complexity.
    **B. Accuracy vs. Speed.**
    C. Speed vs. Simplicity.
    D. Complexity vs. Speed.
    Source:

181. Applying K-Means to PCA-reduced data can be significantly faster, especially for very high-dimensional data, but might result in slightly lower clustering accuracy compared to using the raw data. This is the trade-off mentioned. This statement is:
    A. False.
    **B. True.**
    C. Using PCA-reduced data is always more accurate.
    D. Using raw data is always faster.
    Source:

182. The sources show that when K-Means was applied to the PCA-reduced digit data (2 dimensions), the resulting clusters and background segmentation were visually less distinct and had more mixing of classes compared to when K-Means was applied to the original 64-dimensional data. This visually demonstrates the accuracy trade-off. This statement is:
    A. False.
    **B. True.**
    C. The 2D K-Means result was perfect.
    D. The raw data K-Means result was less accurate.
    Sources:

183. In summary, to visualize multi-class data: if data has labels and is 2D/3D, use a colored scatter plot. If data has labels but is high-dimensional, use PCA to reduce dimensions to 2D/3D, then use a colored scatter plot. This statement is:
    A. False.
    **B. True.**
    C. Labels are not needed for these approaches.
    D. Scatter plots are not suitable for multi-class data.
    Source:

184. If data does NOT have labels, clustering algorithms like K-Means can be used to find groups. You would then visualize the data points colored by their predicted cluster label. This statement is:
    A. False.
    **B. True.**
    C. Clustering is only for labeled data.
    D. You cannot visualize data without original labels.
    Sources:

185. Besides coloring data points or drawing bounding polygons, background segmentation is presented as a third main approach for visualizing multi-class data. It involves coloring the entire visualization space based on predicted class membership. This statement is:
    A. False.
    **B. True.**
    C. Background segmentation only works with bounding polygons.
    D. There are only two main approaches mentioned.
    Sources:

186. When visualizing multi-class data after using PCA for dimensionality reduction, you can visualize the cluster centers (centroids) obtained from K-Means. If K-Means was applied to the reduced 2D data, the centers are already 2D and can be plotted. This statement is:
    A. False.
    **B. True.**
    C. Cluster centers are never visualized.
    D. Centers from 2D data are always 64D.
    Sources:

187. If K-Means was applied to the original high-dimensional data (e.g., 64D), the resulting cluster centers will also be high-dimensional. To visualize these centers on a 2D plot, you must apply PCA to the centers themselves to reduce their dimensions. This statement is:
    A. False.
    **B. True.**
    C. Centers from high-dimensional data are always 2D.
    D. PCA cannot be applied to cluster centers.
    Source:

188. The sources mention the `meshgrid` function in Python as a way to create a grid of points (like the x and y coordinates for background segmentation) from input arrays. This function generates coordinate matrices. This statement is:
    A. False.
    **B. True.**
    C. `meshgrid` is used for clustering, not grid creation.
    D. `meshgrid` creates random points.
    Source:

189. The sources describe taking the coordinate matrices generated by `meshgrid` and 'flattening' them into single vectors. These flattened vectors (one for X coordinates, one for Y) are then combined (stacked) to create a matrix where each row is a point (`x`, `y`) in the grid. This matrix represents all points in the background space. This statement is:
    A. False.
    **B. True.**
    C. `meshgrid` output is used directly without flattening.
    D. The stacked matrix represents only the data points, not the background.
    Sources:

190. This matrix of background grid points is then fed into a classifier or the K-Means prediction function to get a predicted class label for each point. These predicted labels are then used to assign a color to each point on the visualization background. This statement is:
    A. False.
    **B. True.**
    C. The background points are colored randomly.
    D. The prediction function only outputs the distance to centers.
    Sources:

191. A Scatter plot is a visualization where individual data points are plotted using Cartesian coordinates. It's suitable for visualizing the relationship between two numerical variables or the distribution of multi-class data points after dimensionality reduction. This statement is:
    A. False.
    **B. True.**
    C. Scatter plots cannot be used for multi-class data.
    D. Scatter plots are only for categorical data.
    Sources:

192. When comparing different methods for dimensionality reduction or visualization, such as PCA followed by K-Means vs. K-Means on raw data, there can be trade-offs involving computational cost (speed) and the quality or accuracy of the result (e.g., class separation). This requires considering the specific goal and constraints. This statement is:
    A. False.
    **B. True.**
    C. All methods have the same speed and accuracy.
    D. The goal is always perfect accuracy regardless of speed.
    Sources:

193. The sources mention Python libraries like `Matplotlib` for plotting. Functions like `fill` are used to draw polygons. This statement is:
    A. False.
    **B. True.**
    C. Only proprietary software can draw polygons.
    D. `Matplotlib` is only for data analysis, not plotting.
    Sources:

194. Creating animations of the K-Means clustering process can visually demonstrate how the cluster centers move and how data points are reassigned during iterations. This helps understand the algorithm's behavior. This statement is:
    A. False.
    **B. True.**
    C. K-Means is a static algorithm and cannot be animated.
    D. Animation makes the process harder to understand.
    Sources:

195. When calculating the minimum and maximum values for whiskers in a box plot, the formula involves the IQR and a multiplier (e.g., 1.5). Values outside of `(Q1 - 1.5IQR)` and `(Q3 + 1.5IQR)` are potential outliers. This statement is:
    A. False.
    **B. True.**
    C. Outliers are always within the whiskers.
    D. The multiplier (1.5) cannot be changed.
    Sources:

196. An example of calculating the IQR and potential outliers is provided for a sample dataset of weights. The steps include ordering the data, finding the median (Q2), Q1, Q3, IQR, and then calculating the minimum/maximum whisker bounds. This statement is:
    A. False.
    **B. True.**
    C. Ordering the data is not necessary.
    D. Outliers are never excluded from the minimum/maximum.
    Sources:

197. The covariance matrix represents the relationships (covariance) between every pair of random variables in the dataset. Its size is `n x n`, where `n` is the number of dimensions. This statement is:
    A. False.
    **B. True.**
    C. The covariance matrix only shows variance, not covariance.
    D. The covariance matrix is always a `2x2` matrix.
    Sources:

198. In the example calculation of eigenvalues, the process involves setting the determinant of `(A - lambda * I)` to zero and solving the resulting polynomial equation for `lambda`. The roots of this polynomial are the eigenvalues. This statement is:
    A. False.
    **B. True.**
    C. Eigenvalues are found by solving `(A + lambda * I) = 0`.
    D. The determinant is set to 1, not 0.
    Sources:

199. For a `2x2` matrix, the characteristic equation `determinant(A - lambda * I) = 0` results in a quadratic equation for `lambda`. For a `3x3` matrix, it results in a cubic equation. The number of eigenvalues equals the number of dimensions (`n`). This statement is:
    A. False.
    **B. True.**
    C. The number of eigenvalues is always 2.
    D. The equation for `lambda` is always linear.
    Sources:

200. When calculating eigenvectors for a given eigenvalue (`lambda`), you substitute `lambda` into `(A - lambda * I)` and solve the system of linear equations: `(A - lambda * I) * x = 0`. This involves finding a non-zero vector `x` that satisfies the equation. This statement is:
    A. False.
    **B. True.**
    C. The equation is `(A + lambda * I) * x = 0`.
    D. The solution vector `x` must be the zero vector.
    Sources:

201. The sources demonstrate using row operations to simplify the matrix `(A - lambda * I)` to make it easier to solve for the eigenvector components. Techniques include dividing rows by a scalar, swapping rows, and adding/subtracting multiples of rows. This statement is:
    A. False.
    **B. True.**
    C. Only dividing rows is a valid operation.
    D. Row operations are not allowed when finding eigenvectors.
    Sources:

202. The 'E Q Z' form (الاي كيو زيرو) mentioned for simplifying the matrix `(A - lambda * I)` has a specific structure: a block identity matrix (`I`) in the top left, a block zero matrix (`0`) below it, and a block matrix (`Q`) to the right of `I`. The eigenvector components can be easily read from this form. This statement is:
    A. False.
    **B. True.**
    C. The structure is `I`, `0`, and `Q` stacked vertically.
    D. The form does not help in reading the eigenvector.
    Source:

203. In the context of PCA and SVD, it's noted that while libraries exist to compute eigenvalues, eigenvectors, PCA, etc., understanding the underlying mathematical concepts is crucial for data scientists to select appropriate methods, interpret results, and potentially develop new techniques. This statement is:
    A. False.
    **B. True.**
    C. Using libraries is sufficient; understanding the math is not needed.
    D. Only theoretical understanding is required, not practical application.
    Source:

204. PCA preserves variance. The first principal component (PC1) captures the most variance, the second (PC2) captures the second most (orthogonal to PC1), and so on. This statement is:
    A. False.
    **B. True.**
    C. PC1 captures the least variance.
    D. Principal components are not ordered by variance.
    Sources:

205. For the Anscombe's quartet (dataset 4), visualizing the data points individually using a scatter plot was far more informative about the relationships and trends in the data than simply looking at summary statistics or the raw table. This demonstrates the power of visualization. This statement is:
    A. False.
    **B. True.**
    C. Summary statistics provide all the necessary information.
    D. The raw table is the best way to understand this data.
    Sources:

206. The sources mention that while PCA is good for linear relationships, Kernel PCA is used for non-linear distributions. This is achieved by implicitly mapping data to a higher dimension where a linear separation might exist. This statement is:
    A. False.
    **B. True.**
    C. Kernel PCA is only for linear data.
    D. Kernel PCA reduces dimensions before applying a kernel.
    Sources:

207. SVD is particularly useful for matrix factorization in applications like recommender systems. This often involves sparse matrices (many zeros) representing user ratings or item interactions. This statement is:
    A. False.
    **B. True.**
    C. SVD is not used in recommender systems.
    D. Recommender system data is always dense.
    Sources:

208. The definition of a Categorical variable (Nominal data) is provided as a variable that can take on one of a limited, fixed number of possible values, and does not imply any ordering. An example might be Country or color. This statement is:
    A. False.
    **B. True.**
    C. Categorical variables have a natural order.
    D. Categorical variables take continuous values.
    Sources:

209. Ordinal data is similar to categorical data but has a clear ordering or rank among the possible values. An example given was Year. This statement is:
    A. False.
    **B. True.**
    C. Ordinal data has no implied order.
    D. Ordinal data is always numerical.
    Sources:

210. Quantitative (Numerical) data are variables where the values have a numerical meaning and can be measured on a scale. They can be Discrete or Continuous. This statement is:
    A. False.
    **B. True.**
    C. Quantitative data only includes categorical values.
    D. Quantitative data cannot be measured on a scale.
    Sources:

211. Discrete data is countable and takes specific, often integer, values. Continuous data can take any value within a given range, with infinite possibilities between values. This statement is:
    A. False.
    **B. True.**
    C. Discrete data is always continuous.
    D. Continuous data can only take whole numbers.
    Sources:

212. The sources illustrate the concept of variance by comparing two sets of numbers that have the same mean but different spreads. This emphasizes that mean alone does not fully describe a distribution. This statement is:
    A. False.
    **B. True.**
    C. Sets with the same mean always have the same variance.
    D. Variance is not important for understanding distribution.
    Sources:

213. The explained variance ratio associated with each principal component (eigenvalue) indicates the proportion of the total dataset's variance captured by that component. Summing the explained variances for the chosen components gives the total variance preserved. This statement is:
    A. False.
    **B. True.**
    C. Explained variance ratio indicates the mean captured.
    D. Summing explained variances does not give total variance preserved.
    Sources:

214. When applying PCA, the goal is often to select a subset of the principal components (k) that capture a high percentage of the total variance (e.g., 95-99%) while significantly reducing the number of dimensions. This allows for dimensionality reduction with minimal loss of important information. This statement is:
    A. False.
    **B. True.**
    C. PCA always aims for exactly 50% explained variance.
    D. The number of components is always chosen randomly.
    Sources:

215. The Reconstruction process in dimensionality reduction allows you to reverse the transformation and approximate the original data from the lower-dimensional representation. This capability is mentioned in the context of Kernel PCA. This statement is:
    A. False.
    **B. True.**
    C. Reconstruction is not possible after dimensionality reduction.
    D. Reconstruction perfectly recovers the original data.
    Source:

216. The sources briefly list Non-Negative Matrix Factorization (NMF) as a decomposition algorithm. This is listed alongside PCA, Kernel PCA, and SVD. This statement is:
    A. False.
    **B. True.**
    C. NMF is a manifold learning algorithm.
    D. NMF is a discriminant analysis algorithm.
    Source:

217. The sources briefly list Spectral Embedding and Locally Linear Embedding (LLE) as manifold learning algorithms. These are listed alongside `t-SNE`. This statement is:
    A. False.
    **B. True.**
    C. Spectral Embedding is a decomposition algorithm.
    D. LLE is a discriminant analysis algorithm.
    Source:

218. The sources briefly list Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) as discriminant analysis algorithms. This statement is:
    A. False.
    **B. True.**
    C. LDA is a decomposition algorithm.
    D. QDA is a manifold learning algorithm.
    Source:

219. Decomposition algorithms (like PCA, SVD, NMF) and Manifold learning algorithms (like `t-SNE`, Spectral Embedding, LLE) are generally categorized as unsupervised methods for dimensionality reduction. Discriminant Analysis methods (like LDA, QDA) are supervised. This statement is:
    A. False.
    **B. True.**
    C. PCA is a supervised method.
    D. LDA is an unsupervised method.
    Source:

220. The Iris dataset is presented as an example of a dataset suitable for classification and visualization using PCA because its classes are separable in the feature space. The clear visual separation in the PCA plot suggests that the features are informative for distinguishing between flower types. This statement is:
    A. False.
    **B. True.**
    C. The Iris dataset features are not useful for classification.
    D. PCA makes the Iris classes completely overlap.
    Sources:

221. The variance spread on the first axis (corresponding to the largest eigenvalue) in PCA represents the direction with the highest variance in the data. This is visualized in one example with a value and a percentage of total variance. This statement is:
    A. False.
    **B. True.**
    C. The first axis always has the lowest variance.
    D. Variance spread is unrelated to eigenvalues.
    Sources:

222. When applying K-Means clustering to unlabelled multi-class data, the algorithm assigns a predicted cluster label to each data point. This is done iteratively by finding initial centers, assigning points, and updating centers until convergence. This statement is:
    A. False.
    **B. True.**
    C. K-Means requires pre-assigned labels.
    D. K-Means only runs for one iteration.
    Sources:

223. The K-Means algorithm starts with some initial cluster centers. These can be chosen randomly or using specific methods. The algorithm refines these centers over iterations. This statement is:
    A. False.
    **B. True.**
    C. Initial centers are not part of K-Means.
    D. Cluster centers are fixed from the beginning.
    Sources:

224. K-Means determines the cluster assignment for each point based on its distance to the current cluster centers. A point is assigned to the cluster whose center is closest. This statement is:
    A. False.
    **B. True.**
    C. Points are assigned based on color, not distance.
    D. Points are assigned to the furthest center.
    Sources:

225. The source describes calculating distances between points and cluster centers by summing the squared differences between their coordinates. The square root is not necessary for simply comparing which distance is smallest. This statement is:
    A. False.
    **B. True.**
    C. The square root is always needed to compare distances.
    D. Distance calculation is not part of K-Means.
    Sources:

226. After points are assigned to clusters in K-Means, the new cluster centers are computed as the mean of all points assigned to that cluster. This step updates the centers based on the current assignments. This statement is:
    A. False.
    **B. True.**
    C. New centers are chosen randomly.
    D. Centers are not updated in K-Means.
    Source:

227. The K-Means algorithm repeats the steps of assigning points to the nearest center and updating centers until convergence. Convergence occurs when the cluster assignments no longer change significantly or the centers stabilize. This statement is:
    A. False.
    **B. True.**
    C. K-Means runs for a fixed number of iterations, regardless of convergence.
    D. The process stops only when all points are perfectly separated.
    Sources:

228. K-Means is typically used for data without initial labels to automatically group similar data points into clusters. These resulting clusters can then be treated as categories or classes for visualization or further analysis. This statement is:
    A. False.
    **B. True.**
    C. K-Means is a supervised classification algorithm.
    D. K-Means cannot assign labels to data points.
    Sources:

229. The digit dataset is used as an example of multi-class data with a relatively high number of classes (10 digits, 0-9) and high-dimensional features (64 features per digit image). This makes it suitable for demonstrating dimensionality reduction and clustering techniques. This statement is:
    A. False.
    **B. True.**
    C. The digit dataset has only 2 classes.
    D. The digit dataset has low-dimensional features.
    Sources:

230. The sources show how to programmatically determine the number of classes in a labeled dataset by finding the unique values among the labels and counting them. This is useful when the number of classes isn't immediately known. This statement is:
    A. False.
    **B. True.**
    C. The number of classes must always be known manually beforehand.
    D. Unique values of features, not labels, determine class count.
    Source:

231. The three main visualization approaches for multi-class data summarized are: coloring individual data points (e.g., scatter plot), drawing bounding shapes/polygons around clusters, and background segmentation. This statement is:
    A. False.
    **B. True.**
    C. Only two approaches were discussed.
    D. Tables are a primary approach for multi-class visualization.
    Sources:

232. When dealing with high-dimensional data, the general workflow for visualization involves applying dimensionality reduction first (e.g., PCA) to get 2D or 3D data, and then applying a visualization method suitable for 2D/3D. This statement is:
    A. False.
    **B. True.**
    C. Visualization should always happen before dimensionality reduction.
    D. Dimensionality reduction always results in 1D data.
    Source:

233. The benefit of background segmentation for multi-class visualization is that it clearly shows the decision boundaries learned by a classification or clustering model across the entire visualization space. This helps understand how the model separates the classes. This statement is:
    A. False.
    **B. True.**
    C. Background segmentation hides the decision boundaries.
    D. It only shows the individual data points.
    Sources:

234. The accuracy of the decision boundaries shown by background segmentation depends on the model used (classification or clustering) and the density of the background grid. A more accurate model and denser grid yield a more precise representation of the boundaries. This statement is:
    A. False.
    **B. True.**
    C. Grid density does not affect the accuracy of the boundaries.
    D. The model accuracy is irrelevant for background segmentation.
    Sources:

235. The sources demonstrate using Python code snippets involving `NumPy` for mathematical operations (like mean calculation for standardization or matrix operations). This shows the practical implementation aspects. This statement is:
    A. False.
    **B. True.**
    C. Only theoretical concepts were presented.
    D. No specific libraries were mentioned for implementation.
    Sources:

236. The sources discuss how to interpret the K-Means output, which includes the predicted labels for each data point and the coordinates of the final cluster centers. These outputs are used for visualization. This statement is:
    A. False.
    **B. True.**
    C. K-Means output is not used for visualization.
    D. Only the labels are output by K-Means.
    Sources:

237. In the context of K-Means clustering, the term "centroid" refers to the center of a cluster, calculated as the mean of all data points assigned to that cluster. This each cluster and connecting them. This creates a shape that encloses all points within that cluster. This statement is:
    A. False.
    **B. True.**
    C. Polygons are drawn randomly around points.
    D. Polygons connect every point in the cluster.
    Source:

238. The source mentions that the order of points used to draw a polygon using a function like `Matplotlib`'s `fill` matters for creating the correct shape. Points are typically ordered in a sequence around the boundary, such as counter-clockwise. This statement is:
    A. False.
    **B. True.**
    C. The order of points doesn't affect the polygon shape.
    D. Points are always ordered clockwise.
    Sources:

239. The 'explained variance' concept in PCA helps decide how many principal components to keep. You might choose enough components to capture a certain percentage of the total variance (e.g., 95%). This statement is:
    A. False.
    **B. True.**
    C. Explained variance is used to choose the mean.
    D. You always keep all components regardless of explained variance.
    Sources:

240. The Eigenvalues represent the amount of variance captured by their corresponding eigenvectors. Larger eigenvalues indicate directions with more variance. This statement is:
    A. False.
    **B. True.**
    C. Eigenvalues represent the mean.
    D. Larger eigenvalues mean less variance.
    Sources:

241. The concept of "Dimensionality Reduction" is introduced to address issues with high-, allowing viewers to understand and interpret the data to make informed choices. This statement is:
    A. False.
    **B. True.**
    C. Visualization only tells stories, it doesn't support decisions.
    D. Only experts can use visualizations for decision-making.
    Sources:

242. Preattentive attributes are processed by the brain very quickly, before conscious attention is fully engaged. This makes them useful for directing the viewer's eye to important elements in a visualization. This statement is:
    A. False.
    **B. True.**
    C. Preattentive attributes require significant processing time.
    D. They are not used to guide attention.
    Source:

243. The importance of using color appropriately in data visualization is stressed, noting that it is frequently misused. Color should have a purpose, not just be added for decoration. This statement is:
    A. False.
    **B. True.**
    C. Color is rarely misused in visualizations.
    D. Adding color for decoration is always recommended.
    Source:

244. The sources define Sequential Color as using varying shades of a single color to represent quantitative values, typically from light (low value) to dark (high value). This is suitable for ordered numerical data. This statement is:
    A. False.
    **B. True.**
    C. Sequential color uses many different colors.
    D. It is used for categorical data.
    Source:

245. Highlight color is used to make specific elements stand out without necessarily implying a negative or alarming status. Examples include highlighting a data point, line, or text. This statement is:
    A. False.
    **B. True.**
    C. Highlight color is the same as alarm color.
    D. Highlight color is only for negative information.
    Sources:

246. Challenges in visualizing multi-class data include dealing with visual clutter, selecting methods.
    A. False.
    **B. True.**
    C. Histograms are for categorical data.
    D. Histograms show the relationship between two variables.
    Source:

247. Box plots provide a visual summary of a dataset's distribution, showing key statistics like the median, quartiles, and potential outliers. They are useful for comparing distributions across different groups. This statement is:
    A. False.
    **B. True.**
    C. Box plots hide the median.
    D. Box plots only show the minimum and maximum.
    Sources:

248. Scatter plots are ideal for visualizing the relationship between two continuous numerical variables. Each point represents an observation with its values for both variables. This statement is:
    A. False.
    **B. True.**
    C. Scatter plots are for single variables.
    D. Scatter plots are only for categorical data.
    Sources:

249. Bubble charts extend scatter plots to include a third quantitative variable, which is represented by the size of the data point (bubble). This allows visualizing three dimensions simultaneously on a 2D plane. This statement is:
    A. False.
    **B. True.**
    C. Bubble charts use color for the third dimension.
    D. Bubble charts can only visualize two dimensions.
    Sources:

250. Heat maps represent the values in a matrix using variations in color. They are useful for visualizing patterns in complex data matrices, such as correlation matrices or data indexed by two categorical/ordinal variables. This statement is:
    A. False.
    **B. True.**
    C. Chart selection is arbitrary.
    D. All charts serve the same purpose.
    Sources:

251. Tables are best for comparing precise, individual values and when multiple units of measure are involved, prioritizing the display of exact numbers over trends. This statement is:
    A. False.
    **B. True.**
    C. Tables are primarily for showing trends.
    D. Tables cannot display multiple units of measure.
    Source:

252. Column charts and cluster column charts are suited for displaying and comparing the rank of values and focusing on extremes, especially with a limited number of categories (less than seven) and short labels. This statement is:
    A. False.
    **B. True.**
    C. They are best for long labels.
    D. They are suitable for visualizing over 15 categories.
    Source:

253. Bar charts are useful for showing goal attainment, comparing values across a moderate number of categories (over 7 but less than 15), handling long category labels, and displaying negative values. This statement is:
    A. False.
    **B. True.**
    C. Bar charts cannot display negative numbers.
    D. Bar charts are only for short labels.
    Source:

254. Radar charts are useful for comparing multiple items or groups on several quantitative or qualitative attributes. They are effective for showing performance profiles across dimensions, typically with 3 to 10 attributes. This statement is:
    A. False.
    **B. True.**
    C. Radar charts are only for showing trends over time.
    D. Radar charts are best with only one or two attributes.
    Source:

255. Pie charts are compositional charts that show parts of a whole in percentages. Each slice represents a proportion, and all slices combined must sum to 100%. They are useful for showing relative size contributions.
    A. False.
    **B. True.**
    C. Stacked bar charts do not show composition.
    Source:

256. Dimensionality reduction helps address issues like difficulty clustering similar features in high dimensions, increased space and time complexity, the risk of overfitting in machine learning models, and the inability to visualize the data. This statement is:
    A. False.
    **B. True.**
    C. Dimensionality reduction increases overfitting risk.
    D. High dimensions make clustering easier.
    Source:

257. PCA is an unsupervised, linear dimensionality reduction technique that finds a new set of uncorrelated variables (principal components) that capture the maximum variance in the original data. The first principal component captures the most variance. This statement is:
    A. False.
    **B. True.**
    C. PCA is a supervised method.
    D. PCA only works on non-linear data.
    Sources:

258. SVD is a matrix decomposition technique particularly useful for sparse data, such as those found in recommender systems or text analysis (bag of words). It decomposes a matrix A into three other matrices: `A = USV^T`. This statement is:
    A. False.
    **B. True.**
    C. SVD is best for dense data.
    D. SVD is not used in recommender systems.
    Sources:

259. PCA and SVD are related techniques, both being linear dimensionality reduction methods. PCA can be implemented using SVD on the covariance matrix, although the core calculation approach differs slightly.
    A. False.
    **B. True.**
    C. High-dimensional data can be directly plotted in 2D/3D.
    D. Dimensionality reduction increases dimensions for plotting.
    Sources:

260. If multi-class data does not have pre-assigned labels Visualizing K-Means clustering results can involve plotting the data points colored by their cluster assignment, plotting the cluster centers (centroids), drawing bounding shapes around clusters, or using background segmentation. This statement is:
    A. False.
    **B. True.**
    C. Only data points can be visualized after clustering.
    D. Cluster centers are never plotted.
    Sources:

261. The background segmentation visualization approach for multi-class data provides a clear picture of the regions in the visualization space that correspond to each class according to a learned model. It colors the entire background based on predicted class. The choice depends on the specific application's needs. This statement is:
    A. False.
    **B. True.**
    C. Raw data is always faster and more accurate.
    D. PCA reduction has no impact on speed or accuracy.
    Sources:

262. Understanding the underlying mathematical concepts of dimensionality reduction techniques like PCA (eigenvalues, eigenvectors, covariance) is emphasized as important for data scientists, beyond just knowing how to use library functions. This knowledge helps in interpreting results and selecting methods. This statement is:
    A. False.
    **B. True.**
    C. Only practical application of libraries is necessary.
    D. Mathematical understanding is only for theoreticians.
    Sources:

263. Preattentive attributes are processed by the brain extremely fast, within milliseconds. They include features like color, size, and position. This enables viewers to quickly identify patterns or outliers without focused attention. This statement is:
    A. False.
    **B. True.**
    C. They are processed slowly.
    D. They require deliberate focus.
    Source:

264. The importance of using color strategically in data visualization is highlighted, with warnings against using it solely for decoration. Misusing color can hinder understanding or mislead the viewer.
    A. False.
    **B. True.**
    C. CVD does not affect visualization accessibility.
    D. Red and green are universally distinguishable colors.
    Sources:

265. Histograms and box plots are types of charts primarily used to visualize the distribution of a single quantitative variable. They summarize how values are spread across a range. This statement is:
    A. False.
    **B. True.**
    C. Scatter plots are only for showing distributions.
    D. They cannot show correlation.
    Sources:

266. Bubble charts add a third quantitative dimension to a scatter plot by varying the size of the plotted points. This allows for the visualization of three numerical variables simultaneously. This statement is:
    A. False.
    **B. True.**
    C. Bubble charts use color for the third dimension.
    D. Bubble charts visualize only two dimensions.
    Sources:

267. Heat maps visually represent matrix data by using color intensity or variation to depict the value in each cell. They are useful for identifying patterns or hotspots in grid-like data structures. This statement is:
    A. False.
    **B. True.**
    C. Chart selection is arbitrary.
    D. All charts serve the same purpose.
    Sources:

268. Composition charts like Pie charts and Stacked bar charts are used to show how parts contribute to a whole. Pie charts are best for simple compositions summing to 100%, while stacked bars can compare compositions across multiple items. This statement is:
    A. False.
    **B. True.**
    C. Composition charts show trends over time.
    D. Stacked bars cannot show composition.
    Source:

269. Dimensionality reduction techniques are essential for analyzing and visualizing datasets with a large number of features, helping to avoid issues like the curse of dimensionality, overfitting, and difficulty in visualizing high-dimensional spaces. This statement is:
    A. False.
    **B. True.**
    C. Principal components are always correlated.
    D. PCA only works on non-linear data.
    Sources:

270. Kernel PCA extends PCA to handle non-linear data by mapping it into a higher-dimensional space where linear separation becomes possible, using kernel functions. This allows applying PCA concepts to complex data structures. This statement is:
    A. False.
    **B. True.**
    C. SVD requires dense matrices.
    D. SVD is not used in feature extraction.
    Sources:

271. Eigenvalues and eigenvectors are fundamental concepts in linear algebra used in PCA. Eigenvectors represent directions. Finding eigenvectors involves solving the system `(A - λI)x = 0` for each eigenvalue `λ`. These calculations are key to PCA. This statement is:
    A. False.
    **B. True.**
    C. Eigenvectors are found before eigenvalues.
    D. The equation is `det(A + λI) = 0`.
    Source:

272. The covariance matrix summarizes the variance of each feature and the covariance between pairs
    *(This question is incomplete and cannot be answered as formatted.)*

273. When visualizing multi-class data, especially without labels, clustering algorithms like K-Means can group data points based on similarity. The results can then be visualized by coloring points according to their assigned cluster. This statement is:
    A. False.
    **B. True.**
    C. Clustering requires labels.
    D. Clustering is not used for visualization.
    Sources:

274. Visualizing K-Means results can include plotting cluster centers (centroids), drawing bounding polygons around clusters, or using background segmentation to show the decision boundaries implied by the clusters. These methods help interpret the clustering structure
    *(This question is incomplete and cannot be answered as formatted.)*

275. Background segmentation for multi-class data involves creating a grid over the visualization space and predicting the class for each grid point using a model. The background is then colored according to these predictions, illustrating the model's class regions. This statement is:
    A. False.
    **B. True.**
    C. PCA always improves accuracy and speed.
    D. Using raw data is always faster.
    Sources:

276. The number of eigenvalues for an `n x n` matrix is equal to `n`. Each eigenvalue corresponds to a specific eigenvector, and they are calculated from the characteristic equation. In SVD decomposition `A = USV^T`, the diagonal elements of `Σ` are the singular values, which correspond to the "strength" of the concepts captured. `U` contains the left singular vectors, and `V` contains the right singular vectors. This statement is:
    A. False.
    **B. True.**
    C. `Σ` contains eigenvectors.
    D. `U` and `V` are diagonal matrices.
    Sources:

277. When plotting data over time, a Line chart is generally the most appropriate chart type to show trends. Multiple lines can be used to compare trends for different categories. This statement is:
    A. False.
    **B. True.**
    C. Bar charts are best for showing trends over time.
    D. Line charts cannot show trends for multiple categories.
    Sources:

278. For comparing discrete items based on a
    *(This question is incomplete and cannot be answered as formatted.)*

279. When choosing a chart type, questions about the type of data (categorical, ordinal, quantitative), the intended message, the audience, and the desired insights are critical considerations. This structured approach helps select the most effective visualization. This statement is:
    A. False.
    **B. True.**
    C. Only the data type matters for chart selection.
    D. The audience is not a factor in chart choice.
    Source:

280. The usefulness of Tables lies in their ability to present precise, individual values and handle multiple units of measure, especially when the focus is on exact figures rather than visual trends. This statement is:
    A. False.
    **B. True.**
    C. Tables are primarily graphical tools.
    D. Tables cannot show exact quantitative data.
    Source:

281. Radar charts are useful for comparing the performance or characteristics of multiple entities across various attributes. They plot values along radial axes, forming a polygonal. This statement is:
    A. False.
    **B. True.**
    C. Explained variance measures the mean preserved.
    D. Keeping fewer components increases explained variance.
    Sources:

282. Color Vision Deficiency (CVD) means people cannot distinguish colors in the same way as the rest of the population, though they can still see color. This necessitates careful color choices in visualizations to avoid relying on distinctions that some viewers cannot perceive. This statement is:
    A. False.
    **B. True.**
    C. People with CVD are completely colorblind.
    D. Color choice doesn't affect accessibility for CVD.
    Sources:
